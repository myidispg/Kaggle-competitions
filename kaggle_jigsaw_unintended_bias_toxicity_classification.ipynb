{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "kaggle_jigsaw_unintended_bias_toxicity_classification.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/myidispg/Kaggle-competitions/blob/master/kaggle_jigsaw_unintended_bias_toxicity_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "tI7MdCa22H5E",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# A notebook for the Jigsaw Unintended Bias in Toxicity Classification on Kaggle.\n",
        "https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/overview"
      ]
    },
    {
      "metadata": {
        "id": "CUEb8vJe2Y4E",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##All the necessary imports"
      ]
    },
    {
      "metadata": {
        "id": "mI0ZQ6cP2Xup",
        "colab_type": "code",
        "outputId": "a1871be0-188d-4fe8-e136-4ee152c08f8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "from zipfile import ZipFile \n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "PAD_token = 0\n",
        "SOS_token = 1\n",
        "EOS_token = 2\n",
        "\n",
        "dataset_directory = 'kaggle_jigsaw_unintended_bias/'\n",
        "\n",
        "pd.__version__\n",
        "device"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "metadata": {
        "id": "pupnoHH81iwh",
        "colab_type": "code",
        "outputId": "fb6e1a98-1cbd-40cc-d1e6-72e3d4309b16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "os.environ['KAGGLE_USERNAME'] = \"myidispg\"\n",
        "os.environ['KAGGLE_KEY'] = \"c991620902499acb95ee0c402d169f34\"\n",
        "!kaggle competitions download -c jigsaw-unintended-bias-in-toxicity-classification\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_submission.csv.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "test.csv.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "train.csv.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ydt2bkGz26ms",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Check whether the test, train and sample_submission zips have been downloaded."
      ]
    },
    {
      "metadata": {
        "id": "MxM5mRO42BRq",
        "colab_type": "code",
        "outputId": "3902b174-b584-40f1-d5c4-43e88d040d97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print(os.listdir())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['.config', 'kaggle_jigsaw_unintended_bias', 'train.csv.zip', 'test.csv.zip', 'sample_submission.csv.zip', 'sample_data']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cqWYJ6Lh3EPf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Unzip the files and check."
      ]
    },
    {
      "metadata": {
        "id": "zOakS-Yb3Dax",
        "colab_type": "code",
        "outputId": "6d619e96-ff87-4c9d-b229-ab92c095cab2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "cell_type": "code",
      "source": [
        "# unzip the train.csv file\n",
        "train_zip_filename = 'train.csv.zip'\n",
        "with ZipFile(train_zip_filename, 'r') as zip: \n",
        "    # printing all the contents of the zip file \n",
        "    zip.printdir() \n",
        "  \n",
        "    # extracting all the files \n",
        "    print('Extracting all the files now...') \n",
        "    zip.extractall(dataset_directory) \n",
        "    print('Done!')\n",
        "    \n",
        "# unzip the test.csv file\n",
        "train_zip_filename = 'test.csv.zip'\n",
        "with ZipFile(train_zip_filename, 'r') as zip: \n",
        "    # printing all the contents of the zip file \n",
        "    zip.printdir() \n",
        "  \n",
        "    # extracting all the files \n",
        "    print('Extracting all the files now...') \n",
        "    zip.extractall(dataset_directory) \n",
        "    print('Done!') \n",
        "    \n",
        "# unzip the sample_submission.csv file\n",
        "train_zip_filename = 'sample_submission.csv.zip'\n",
        "with ZipFile(train_zip_filename, 'r') as zip: \n",
        "    # printing all the contents of the zip file \n",
        "    zip.printdir() \n",
        "  \n",
        "    # extracting all the files \n",
        "    print('Extracting all the files now...') \n",
        "    zip.extractall(dataset_directory) \n",
        "    print('Done!') "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File Name                                             Modified             Size\n",
            "train.csv                                      2019-03-28 21:17:38    816211476\n",
            "Extracting all the files now...\n",
            "Done!\n",
            "File Name                                             Modified             Size\n",
            "test.csv                                       2019-03-28 15:08:42     30179878\n",
            "Extracting all the files now...\n",
            "Done!\n",
            "File Name                                             Modified             Size\n",
            "sample_submission.csv                          2019-03-28 15:08:42      1167854\n",
            "Extracting all the files now...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UtffJcfP4ktu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Check if the files have been extracted properly"
      ]
    },
    {
      "metadata": {
        "id": "p3znMcc54rZw",
        "colab_type": "code",
        "outputId": "b85e47de-35e6-4cb0-9259-19f5c91a86b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "if set(['train.csv', 'test.csv', 'sample_submission.csv']).issubset(os.listdir(dataset_directory)):\n",
        "  print('The dataset has been extracted properly.')\n",
        "else:\n",
        "  print('There was some issue in dataset download or extraction.')\n",
        " \n",
        "print(f'The contents of the dataset directory are- {os.listdir(dataset_directory)}')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The dataset has been extracted properly.\n",
            "The contents of the dataset directory are- ['sample_submission.csv', 'train.csv', 'test.csv']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LRQKnDI94G0L",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load the training data in a pandas dataframe"
      ]
    },
    {
      "metadata": {
        "id": "yrEAdEZH4Kwp",
        "colab_type": "code",
        "outputId": "8e4efc0b-f9f8-4d6a-d518-48b5384b263a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "cell_type": "code",
      "source": [
        "df_train = pd.read_csv(dataset_directory + 'train.csv')\n",
        "# print(f'The first entry in train dataframe is: \\n{df_train[:1]}\\n')\n",
        "print(f'The column names in train dataframe are- {list(df_train.columns.values)}\\n\\n')\n",
        "\n",
        "df_test = pd.read_csv(dataset_directory + 'test.csv')\n",
        "# print(f'The first entry in test dataframe is: \\n{df_train[:1]}\\n')\n",
        "print(f'The column names in test dataframe are- {list(df_test.columns.values)}')\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The column names in train dataframe are- ['id', 'target', 'comment_text', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat', 'asian', 'atheist', 'bisexual', 'black', 'buddhist', 'christian', 'female', 'heterosexual', 'hindu', 'homosexual_gay_or_lesbian', 'intellectual_or_learning_disability', 'jewish', 'latino', 'male', 'muslim', 'other_disability', 'other_gender', 'other_race_or_ethnicity', 'other_religion', 'other_sexual_orientation', 'physical_disability', 'psychiatric_or_mental_illness', 'transgender', 'white', 'created_date', 'publication_id', 'parent_id', 'article_id', 'rating', 'funny', 'wow', 'sad', 'likes', 'disagree', 'sexual_explicit', 'identity_annotator_count', 'toxicity_annotator_count']\n",
            "\n",
            "\n",
            "The column names in test dataframe are- ['id', 'comment_text']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ugp_v8VHOG40",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Convert the dataframes to numpy arrays\n",
        "Convert the dataframes to numpy arrays. Also, the id, target and comment_text columns of the train dataset are preserved. The others are discarded as of now."
      ]
    },
    {
      "metadata": {
        "id": "CNE3FEKSOLbg",
        "colab_type": "code",
        "outputId": "4012fc3c-025b-491f-8cf5-d90a47260fae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "cell_type": "code",
      "source": [
        "df_train = df_train.iloc[:, :3]\n",
        "np_train = df_train.to_numpy()\n",
        "print(np_train[100])\n",
        "np_test = df_test.to_numpy()\n",
        "print(np_test[100])\n",
        "\n",
        "del df_train, df_test"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[239722 0.0\n",
            " 'Loving this collection. Cant wait till Season 2 is released. Should be any day now according to http://yeezy-season2.com/']\n",
            "[7000100\n",
            " 'Did you even read the editorial?\\n\\n\"The best course is for the democratic world to continue to demand the return of the countryâ€™s legislature, and the end of the sham constituent assembly that usurped it. Impartial outsiders, working with both the opposition and Mr. Maduro, could help negotiate a schedule for the return of legitimate elections.\"\\n\\nHow exactly do you interpret that as a call for a military invasion?']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OZ_1cBdy-Aeq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Normalize the strings, stem, convert to lower case and remove all punctutations.\n",
        "\n",
        "Will try stemming later. Stemming might result in information loss while the model will try to understand the context of the sentence.\n",
        "\n",
        "Note: The symbol to remove apostrophes was found on- https://stackoverflow.com/questions/44296593/how-to-remove-apostrophe-marks-from-a-string-in-python?rq=1"
      ]
    },
    {
      "metadata": {
        "id": "RtUUC27q-M-G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a0e54a64-6687-4206-f60d-761955d94617"
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "import unicodedata\n",
        "\n",
        "# Turn a Unicode string to plain ASCII, thanks to\n",
        "# https://stackoverflow.com/a/518232/2809427\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "def normalizeAndStemString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "#     s = [word.replace(\"'\", \"\") for word in s.split()]\n",
        "#     for i in range(len(s)):\n",
        "#       s[i] = ps.stem(s[i])\n",
        "    s = re.sub(r\"http[s]?:\\/\\/\\S+\", '', s)\n",
        "    s = s.replace(u\"\\u2019\", \"\") # remove apostrophes\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s) # remove punctuations\n",
        "    s = re.sub(r\"[^a-zA-Z0-9.!?]+\", r\" \", s) # remove all characters except alphabets and .!?\n",
        "    return s\n",
        "\n",
        "print('Normalizing dataset....')\n",
        "for i in range(len(np_test)):\n",
        "  np_test[i][1] = normalizeAndStemString(str(np_test[i][1]))\n",
        "# print(np_test[100])\n",
        "print('Normalized Test dataset')\n",
        "for i in range(len(np_train)):\n",
        "  np_train[i][2] = normalizeAndStemString(str(np_train[i][2]))\n",
        "# print(np_train[100])\n",
        "print('Normalized Train dataset')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Normalizing dataset....\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CagpUE9wT9lv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Extract x_train, y_train, x_valid, y_valid and x_test from the numpy arrays.\n",
        "The train dataset will be split into train and validation set to measure the performance of the model."
      ]
    },
    {
      "metadata": {
        "id": "eGrgR_4EUPgw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_train = np_train[:, 2]\n",
        "y_train = np_train[:, 1]\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.20, shuffle=True)\n",
        "\n",
        "print(f'The len of the train set is: {x_train.shape[0]}')\n",
        "print(f'The len of the validation set is: {x_valid.shape[0]}')\n",
        "\n",
        "y_train = np.where(y_train >= 0.5, 1, 0)\n",
        "y_valid = np.where(y_valid >= 0.5, 1, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WpDWUw42Vkp-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Now that we have the train, valid and test set, we can now work on creating a vocabulary for all the comments.\n",
        "The vocabulary will have word2idx, idx2word, word2count, max_sent_length and n_words."
      ]
    },
    {
      "metadata": {
        "id": "ENeGV-3MWKvz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Vocab:\n",
        "  \n",
        "  def __init__(self):\n",
        "    self.word2index = {}\n",
        "    self.word2count = {}\n",
        "    self.index2word = {0: \"PAD\", 1: \"SOS\", 2: \"EOS\"}\n",
        "    self.n_words = 2\n",
        "    self.max_sent_length = 0\n",
        "    \n",
        "  def addSentence(self, sentence):\n",
        "    sent_length = len(sentence.split(' '))\n",
        "    self.max_sent_length = sent_length if sent_length > self.max_sent_length else self.max_sent_length        \n",
        "    for word in sentence.split(' '):\n",
        "      self.addWord(word)\n",
        "      \n",
        "  def addWord(self, word):\n",
        "    if word not in self.word2index:\n",
        "      self.word2index[word] = self.n_words\n",
        "      self.word2count[word] = 1\n",
        "      self.index2word[self.n_words] = word\n",
        "      self.n_words += 1\n",
        "    else:\n",
        "      self.word2count[word] += 1\n",
        "      \n",
        "      \n",
        "vocab = Vocab()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qpcXRDDfXJTS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Go through all the sentences in the train, validation and test set and add to vocabulary.\n"
      ]
    },
    {
      "metadata": {
        "id": "L_FfupdyXRvZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Add all words from test set to vocabulary.\n",
        "for line in np_test:\n",
        "  vocab.addSentence(line[1])\n",
        "  \n",
        "print(f'Number of words in vocabulary after adding test set: {vocab.n_words}')\n",
        "\n",
        "# Add all words from train set to vocabulary.\n",
        "for line in x_train:\n",
        "  vocab.addSentence(line)\n",
        "  \n",
        "print(f'Number of words in vocabulary after adding train set: {vocab.n_words}')\n",
        "\n",
        "# Add all words from validation set to vocabulary.\n",
        "for line in x_valid:\n",
        "  vocab.addSentence(line)\n",
        "  \n",
        "print(f'Number of words in vocabulary after adding validation set: {vocab.n_words}')\n",
        "\n",
        "MAX_LENGTH = vocab.max_sent_length\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XsCTs6ExZqsB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Now that the vocabulary is ready, moving onto some Exploratory Data Analysis.\n",
        "In this section, I will get the-\n",
        "1. Number of samples\n",
        "2. Number of samples per class(toxic or non-toxic)\n",
        "3. Median number of words per sample"
      ]
    },
    {
      "metadata": {
        "id": "1umBKcB0aY8Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(f'The total number of combined samples of the train and validation set are: {x_train.shape[0] + x_valid.shape[0]}')\n",
        "\n",
        "# ------Calculate number of samples per class-----------------------------------\n",
        "num_toxic_samples = 0\n",
        "num_non_toxic_samples = 0\n",
        "\n",
        "for entry in y_train:\n",
        "  if entry > 0.5:\n",
        "    num_toxic_samples += 1\n",
        "  else:\n",
        "    num_non_toxic_samples += 1\n",
        "    \n",
        "for entry in y_valid:\n",
        "  if entry > 0.5:\n",
        "    num_toxic_samples += 1\n",
        "  else:\n",
        "    num_non_toxic_samples += 1\n",
        "  \n",
        "print(f'The number of toxic samples are: {num_toxic_samples}')\n",
        "print(f'The number of non-toxic samples are: {num_non_toxic_samples}')\n",
        "\n",
        "plt.pie([num_toxic_samples, num_non_toxic_samples], labels=['Toxic samples', 'Non-toxic samples'], autopct='%1.1f%%',)\n",
        "plt.show()\n",
        "# ------------------------------------------------------------------------------\n",
        "\"\"\"By the output of the pie chart, it will be clear that the Toxic comments are very less in number than non-toxic comments.\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9thsqblvlWKf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#----- Find number of words per sample------------------------------------------\n",
        "num_words = []\n",
        "\n",
        "for line in np_test:\n",
        "  num_words.append(len(str(line).split(' ')))\n",
        "  \n",
        "for line in x_train:\n",
        "  num_words.append(len(line.split(' ')))\n",
        "  \n",
        "for line in x_valid:\n",
        "  num_words.append(len(line.split(' ')))\n",
        "  \n",
        "import statistics\n",
        "\n",
        "print(f'The median of the sentence lengths is: {statistics.median(num_words)}')\n",
        "print(f'The maximum sentence length is: {vocab.max_sent_length}')\n",
        "#-------------------------------------------------------------------------------"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eTfyhEjPsuZ8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Creating sequences from words.\n",
        "Some functions to create sequences of indexes for a given sentence. The output will be a Pytorch tensor."
      ]
    },
    {
      "metadata": {
        "id": "QJm_Fmons4be",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# x_train before creating sequences\n",
        "print(x_train[2])\n",
        "\n",
        "def indexesFromSentence(sentence):\n",
        "    return [vocab.word2index[word] for word in sentence.split(' ')]\n",
        "    \n",
        "def tensorFromSentence(sentence):\n",
        "    indexes = indexesFromSentence(sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    if len(indexes) < MAX_LENGTH:\n",
        "      for i in range(len(indexes), MAX_LENGTH):\n",
        "        indexes.append(PAD_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "print(f'Making sure the tensors are of max_length. The shape should be [{MAX_LENGTH}, 1]: {tensorFromSentence(x_train[2]).shape}')\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}