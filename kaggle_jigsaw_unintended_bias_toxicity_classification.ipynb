{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "kaggle_jigsaw_unintended_bias_toxicity_classification.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/myidispg/Kaggle-competitions/blob/master/kaggle_jigsaw_unintended_bias_toxicity_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "tI7MdCa22H5E",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# A notebook for the Jigsaw Unintended Bias in Toxicity Classification on Kaggle.\n",
        "https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/overview"
      ]
    },
    {
      "metadata": {
        "id": "CUEb8vJe2Y4E",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##All the necessary imports"
      ]
    },
    {
      "metadata": {
        "id": "mI0ZQ6cP2Xup",
        "colab_type": "code",
        "outputId": "e906e8d8-1541-4b37-8286-6846418e5ef7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "PAD_token = 0\n",
        "SOS_token = 1\n",
        "EOS_token = 2\n",
        "UNK_token = 4\n",
        "\n",
        "MAX_LENGTH = 220\n",
        "\n",
        "dataset_directory = 'kaggle_jigsaw_unintended_bias/'\n",
        "\n",
        "print(pd.__version__)\n",
        "print(device)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.23.4\n",
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FuELSazr6AjM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Mount Google Drive. It will be used for saving models later\n",
        "The drive will be mounted at the path: '/content/drive/My Drive/"
      ]
    },
    {
      "metadata": {
        "id": "e78gxM_D6HHh",
        "colab_type": "code",
        "outputId": "3bdb6585-db90-44c6-eb92-5ab3c5315a7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "af93cHQQ6Quu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Download the Kaggle dataset."
      ]
    },
    {
      "metadata": {
        "id": "pupnoHH81iwh",
        "colab_type": "code",
        "outputId": "dae35101-d897-420a-eb3d-57b95eecf71e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "cell_type": "code",
      "source": [
        "os.environ['KAGGLE_USERNAME'] = \"myidispg\"\n",
        "os.environ['KAGGLE_KEY'] = \"c991620902499acb95ee0c402d169f34\"\n",
        "!kaggle competitions download -c jigsaw-unintended-bias-in-toxicity-classification\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading sample_submission.csv.zip to /content\n",
            "\r  0% 0.00/221k [00:00<?, ?B/s]\n",
            "100% 221k/221k [00:00<00:00, 30.4MB/s]\n",
            "Downloading test.csv.zip to /content\n",
            " 41% 5.00M/12.1M [00:00<00:00, 17.6MB/s]\n",
            "100% 12.1M/12.1M [00:00<00:00, 34.7MB/s]\n",
            "Downloading train.csv.zip to /content\n",
            " 96% 262M/273M [00:02<00:00, 122MB/s]\n",
            "100% 273M/273M [00:02<00:00, 97.8MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ydt2bkGz26ms",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Check whether the test, train and sample_submission zips have been downloaded."
      ]
    },
    {
      "metadata": {
        "id": "MxM5mRO42BRq",
        "colab_type": "code",
        "outputId": "750bbd78-4de8-44ad-ecae-24b50bf77cf1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print(os.listdir())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['.config', 'glove.6B.zip', 'glove_vectors', 'gdrive', 'kaggle_jigsaw_unintended_bias', 'sample_data']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rvPZxBGO-VC1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Download and extract the glove vector files. \n",
        "Create a list of words, word2idx, and id2emb for the glove vectors."
      ]
    },
    {
      "metadata": {
        "id": "r2Gn3RxeYzCg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "488b0628-eab0-4739-d928-66c00a224524"
      },
      "cell_type": "code",
      "source": [
        "glove_url = 'http://nlp.stanford.edu/data/glove.6B.zip'\n",
        "\n",
        "from urllib.request import urlretrieve\n",
        "urlretrieve(glove_url, 'glove.6B.zip')\n",
        "\n",
        "print('Glove embeddings downloaded.\\nExtracting the files now...')\n",
        "with zipfile.ZipFile('glove.6B.zip', 'r') as zip_ref:\n",
        "  zip_ref.extractall('glove_vectors')\n",
        "\n",
        "print(os.listdir('glove_vectors/')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Glove embeddings downloaded.\n",
            "Extracting the files now...\n",
            "['glove.6B.200d.txt', 'glove.6B.100d.txt', 'glove.6B.50d.txt', 'glove.6B.300d.txt']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "59KR05iQirfh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5e83852a-483d-4b52-c0f3-d942db07f741"
      },
      "cell_type": "code",
      "source": [
        "words = []\n",
        "idx = 0\n",
        "word2idx = {}\n",
        "embeddings = []\n",
        "\n",
        "with open('glove_vectors/glove.6B.50d.txt', 'rb') as f:\n",
        "  for l in f:\n",
        "    line = l.decode().split()\n",
        "    word = line[0]\n",
        "    words.append(word)\n",
        "    word2idx[word] = idx\n",
        "    idx += 1\n",
        "    embeddings.append(line[1: ])\n",
        "  f.close()\n",
        "\n",
        "print('Created the required lists and dictionaries of the glove files!!!')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Created the required lists and dictionaries of the glove files!!!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rw3pd6x-inUe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "a3b53bd7-6e11-4282-c673-c112031f1855"
      },
      "cell_type": "code",
      "source": [
        "print(len(word2idx))\n",
        "print(len(words))\n",
        "print(len(embeddings))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "400000\n",
            "400000\n",
            "400000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YgiwhMZQtMXI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Since the glove vectors won't have SOS, EOS and PAD tokens, append them to the last of the words list initialize random vectors for them.\n"
      ]
    },
    {
      "metadata": {
        "id": "auKMTrhbs9QD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b3f17807-9bce-4f8b-ae46-477bfa035d27"
      },
      "cell_type": "code",
      "source": [
        "num_known_words = len(words)\n",
        "\n",
        "PAD_token = len(words) # Since length will always be one index greater than the last index.\n",
        "SOS_token = len(words) + 1\n",
        "EOS_token = len(words) + 2\n",
        "UNK_token = len(words) + 3\n",
        "\n",
        "word2idx['SOS'] = SOS_token\n",
        "word2idx['EOS'] = EOS_token\n",
        "word2idx['PAD'] = PAD_token\n",
        "word2idx['UNK'] = UNK_token\n",
        "\n",
        "words.append('SOS')\n",
        "words.append('EOS')\n",
        "words.append('PAD')\n",
        "words.append('UNK')\n",
        "\n",
        "# Append 3 random vectors to serve as embeddings for SOS, PAD and EOS.\n",
        "for i in range(4):\n",
        "  embeddings.append([random.randrange(-1, 1) for _ in range(50)])\n",
        "  \n",
        "print('Embedded random vectors for SOS, PAD and EOS.')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Embedded random vectors for SOS, PAD and EOS.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3mANHOjfibFR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "7e30d105-6f34-4d67-9c1f-2d648272a98b"
      },
      "cell_type": "code",
      "source": [
        "print(len(word2idx))\n",
        "print(len(words))\n",
        "print(len(embeddings))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "400004\n",
            "400004\n",
            "400004\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cqWYJ6Lh3EPf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Unzip the files and check."
      ]
    },
    {
      "metadata": {
        "id": "zOakS-Yb3Dax",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "24836318-f7fe-445f-d99f-eba69e5ff2fa"
      },
      "cell_type": "code",
      "source": [
        "# unzip the train.csv file\n",
        "train_zip_filename = 'train.csv.zip'\n",
        "with zipfile.ZipFile(train_zip_filename, 'r') as zip: \n",
        "    # printing all the contents of the zip file \n",
        "    zip.printdir() \n",
        "  \n",
        "    # extracting all the files \n",
        "    print('Extracting all the files now...') \n",
        "    zip.extractall(dataset_directory) \n",
        "    print('Done!')\n",
        "    \n",
        "# unzip the test.csv file\n",
        "train_zip_filename = 'test.csv.zip'\n",
        "with zipfile.ZipFile(train_zip_filename, 'r') as zip: \n",
        "    # printing all the contents of the zip file \n",
        "    zip.printdir() \n",
        "  \n",
        "    # extracting all the files \n",
        "    print('Extracting all the files now...') \n",
        "    zip.extractall(dataset_directory) \n",
        "    print('Done!') \n",
        "    \n",
        "# unzip the sample_submission.csv file\n",
        "train_zip_filename = 'sample_submission.csv.zip'\n",
        "with zipfile.ZipFile(train_zip_filename, 'r') as zip: \n",
        "    # printing all the contents of the zip file \n",
        "    zip.printdir() \n",
        "  \n",
        "    # extracting all the files \n",
        "    print('Extracting all the files now...') \n",
        "    zip.extractall(dataset_directory) \n",
        "    print('Done!')\n",
        "    \n",
        "!rm sample_submission.csv.zip\n",
        "!rm test.csv.zip\n",
        "!rm train.csv.zip"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File Name                                             Modified             Size\n",
            "train.csv                                      2019-03-28 21:17:38    816211476\n",
            "Extracting all the files now...\n",
            "Done!\n",
            "File Name                                             Modified             Size\n",
            "test.csv                                       2019-03-28 15:08:42     30179878\n",
            "Extracting all the files now...\n",
            "Done!\n",
            "File Name                                             Modified             Size\n",
            "sample_submission.csv                          2019-03-28 15:08:42      1167854\n",
            "Extracting all the files now...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UtffJcfP4ktu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Check if the files have been extracted properly"
      ]
    },
    {
      "metadata": {
        "id": "p3znMcc54rZw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "7b8e3d01-768c-43d7-b6c8-c710175d9bc9"
      },
      "cell_type": "code",
      "source": [
        "if set(['train.csv', 'test.csv', 'sample_submission.csv']).issubset(os.listdir(dataset_directory)):\n",
        "  print('The dataset has been extracted properly.')\n",
        "else:\n",
        "  print('There was some issue in dataset download or extraction.')\n",
        " \n",
        "print(f'The contents of the dataset directory are- {os.listdir(dataset_directory)}')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The dataset has been extracted properly.\n",
            "The contents of the dataset directory are- ['train.csv', 'sample_submission.csv', 'test.csv']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LRQKnDI94G0L",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load the training data in a pandas dataframe"
      ]
    },
    {
      "metadata": {
        "id": "yrEAdEZH4Kwp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "8ecd7f30-5eef-4d1e-9ba5-0d0f8fcfd7c7"
      },
      "cell_type": "code",
      "source": [
        "df_train = pd.read_csv(dataset_directory + 'train.csv')\n",
        "# print(f'The first entry in train dataframe is: \\n{df_train[:1]}\\n')\n",
        "print(f'The column names in train dataframe are- {list(df_train.columns.values)}\\n')\n",
        "\n",
        "df_test = pd.read_csv(dataset_directory + 'test.csv')\n",
        "# print(f'The first entry in test dataframe is: \\n{df_train[:1]}\\n')\n",
        "print(f'The column names in test dataframe are- {list(df_test.columns.values)}')\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The column names in train dataframe are- ['id', 'target', 'comment_text', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat', 'asian', 'atheist', 'bisexual', 'black', 'buddhist', 'christian', 'female', 'heterosexual', 'hindu', 'homosexual_gay_or_lesbian', 'intellectual_or_learning_disability', 'jewish', 'latino', 'male', 'muslim', 'other_disability', 'other_gender', 'other_race_or_ethnicity', 'other_religion', 'other_sexual_orientation', 'physical_disability', 'psychiatric_or_mental_illness', 'transgender', 'white', 'created_date', 'publication_id', 'parent_id', 'article_id', 'rating', 'funny', 'wow', 'sad', 'likes', 'disagree', 'sexual_explicit', 'identity_annotator_count', 'toxicity_annotator_count']\n",
            "\n",
            "The column names in test dataframe are- ['id', 'comment_text']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NhBz_mQbZzM_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Remove the special characters from the dataset.\n",
        "Credit goes to https://www.kaggle.com/gpreda/jigsaw-fast-compact-solution"
      ]
    },
    {
      "metadata": {
        "id": "0iaZTfFmYVY0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def preprocess(data):\n",
        "    '''\n",
        "    Credit goes to https://www.kaggle.com/gpreda/jigsaw-fast-compact-solution\n",
        "    '''\n",
        "    punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~`\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n",
        "    def clean_special_chars(text, punct):\n",
        "        for p in punct:\n",
        "            text = text.replace(p, ' ')\n",
        "        return text\n",
        "\n",
        "    data = data.astype(str).apply(lambda x: clean_special_chars(x, punct))\n",
        "    return data\n",
        "  \n",
        "df_train['comment_text'] = preprocess(df_train['comment_text'])\n",
        "df_test['comment_text'] = preprocess(df_test['comment_text'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ugp_v8VHOG40",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Convert the dataframes to numpy arrays\n",
        "Convert the dataframes to numpy arrays. Also, the id, target and comment_text columns of the train dataset are preserved. The others are discarded as of now."
      ]
    },
    {
      "metadata": {
        "id": "CNE3FEKSOLbg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "dfb342bf-8287-414d-b387-e03ef7be308d"
      },
      "cell_type": "code",
      "source": [
        "df_train = df_train.iloc[:, :3]\n",
        "np_train = df_train.iloc[:, :].values\n",
        "print(np_train[100])\n",
        "np_test = df_test.iloc[:, :].values\n",
        "print(np_test[100])\n",
        "\n",
        "del df_train, df_test"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[239722 0.0\n",
            " 'Loving this collection  Cant wait till Season 2 is released  Should be any day now according to http   yeezy season2 com ']\n",
            "[7000100\n",
            " 'Did you even read the editorial \\n\\n The best course is for the democratic world to continue to demand the return of the country s legislature  and the end of the sham constituent assembly that usurped it  Impartial outsiders  working with both the opposition and Mr  Maduro  could help negotiate a schedule for the return of legitimate elections  \\n\\nHow exactly do you interpret that as a call for a military invasion ']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TkQ1t7NOr25m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Filter out sentences that are more than 220 words in length\n"
      ]
    },
    {
      "metadata": {
        "id": "r5R9pNMAr2jb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "31923bd9-a57f-472a-c222-445ec9627846"
      },
      "cell_type": "code",
      "source": [
        "np_train_filtered = []\n",
        "np_test_filtered = []\n",
        "\n",
        "print(f'train_set before filtering: {np_train.shape[0]}')\n",
        "\n",
        "print('filtering sentences of train set')\n",
        "for i in range(np_train.shape[0]):\n",
        "  if len(np_train[i][2].split()) <= MAX_LENGTH:\n",
        "    np_train_filtered.append(np_train[i])\n",
        "\n",
        "print('filtering sentences of test set')\n",
        "for i in range(np_test.shape[0]):\n",
        "  if len(np_test[i][1].split()) <= MAX_LENGTH:\n",
        "    np_test_filtered.append(np_test[i])\n",
        "\n",
        "np_train = np.asarray(np_train_filtered)\n",
        "np_test = np.asarray(np_test_filtered)\n",
        "print('Filtered the sentences')\n",
        "print(f'train_set after filtering: {np_train.shape[0]}')\n",
        "\n",
        "del np_train_filtered, np_test_filtered"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_set before filtering: 1804874\n",
            "filtering sentences of train set\n",
            "filtering sentences of test set\n",
            "Filtered the sentences\n",
            "train_set after filtering: 1804864\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OZ_1cBdy-Aeq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Normalize the strings, stem, convert to lower case and remove all punctutations.\n",
        "\n",
        "Will try stemming later. Stemming might result in information loss while the model will try to understand the context of the sentence.\n",
        "\n",
        "Note: The symbol to remove apostrophes was found on- https://stackoverflow.com/questions/44296593/how-to-remove-apostrophe-marks-from-a-string-in-python?rq=1\n",
        "\n",
        "#------Code for this block(REMOVED AS OF NOW. FOUND A FASTER VERSION)--------\n",
        "import re\n",
        "import unicodedata\n",
        "\n",
        "# Turn a Unicode string to plain ASCII, thanks to\n",
        "# https://stackoverflow.com/a/518232/2809427\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "def normalizetring(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "#     s = [word.replace(\"'\", \"\") for word in s.split()]\n",
        "#     for i in range(len(s)):\n",
        "#       s[i] = ps.stem(s[i])\n",
        "    s = re.sub(r\"http[s]?:\\/\\/\\S+\", '', s)\n",
        "    s = s.replace(u\"\\u2019\", \"\") # remove apostrophes\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s) # remove punctuations\n",
        "    s = re.sub(r\"[^a-zA-Z0-9.!?]+\", r\" \", s) # remove all characters except alphabets and .!?\n",
        "    return s\n",
        "\n",
        "print('Normalizing dataset....')\n",
        "for i in range(len(np_test)):\n",
        "  np_test[i][1] = normalizetring(str(np_test[i][1]))\n",
        "# print(np_test[100])\n",
        "print('Normalized Test dataset')\n",
        "for i in range(len(np_train)):\n",
        "  np_train[i][2] = normalizetring(str(np_train[i][2]))\n",
        "# print(np_train[100])\n",
        "print('Normalized Train dataset')"
      ]
    },
    {
      "metadata": {
        "id": "-44yQGeUy1jm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Get the amount of toxic and non-toxic samples in x_train."
      ]
    },
    {
      "metadata": {
        "id": "Up2Biz6dy8H6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "0728e195-98c0-43f0-de36-44e36c42a98d"
      },
      "cell_type": "code",
      "source": [
        "toxic_count = 0\n",
        "non_toxic_count = 0\n",
        "\n",
        "np_train_toxic = []\n",
        "np_train_non_toxic = []\n",
        "\n",
        "for i in range(np_train.shape[0]):\n",
        "  if np_train[i][1] >= 0.5:\n",
        "    toxic_count += 1\n",
        "    np_train_toxic.append(np_train[i])\n",
        "  else:\n",
        "    non_toxic_count += 1\n",
        "    np_train_non_toxic.append(np_train[i])\n",
        "    \n",
        "np_train_toxic = np.asarray(np_train_toxic)\n",
        "np_train_non_toxic = np.asarray(np_train_non_toxic)\n",
        "\n",
        "print(f'The number of toxic samples are: {toxic_count} and non toxic sample count is: {non_toxic_count}')\n",
        "print(f'The first 2 non toxic comments are: {np_train_non_toxic[:2]}\\nThe first 2 toxic are: {np_train_toxic[:2]}')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of toxic samples are: 144333 and non toxic sample count is: 1660531\n",
            "The first 2 non toxic comments are: [[59848 0.0\n",
            "  'This is so cool  It s like   would you want your mother to read this    Really great idea  well done ']\n",
            " [59849 0.0\n",
            "  'Thank you   This would make my life a lot less anxiety inducing  Keep it up  and don t let anyone get in your way ']]\n",
            "The first 2 toxic are: [[59856 0.8936170212765957 'haha you guys are a bunch of losers ']\n",
            " [59859 0.6666666666666666 'ur a sh tty comment ']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mvbPspFcyLxz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##According to some data analysis earlier, the dataset was heavily biased to non-toxic comments. \n",
        "So, while training, I will generate x_train as many as times as the number of epochs.\n",
        "Each time, the generated x_train will have balanced amount of toxic and non -toxic samples. The non-toxic samples will be picked randomly."
      ]
    },
    {
      "metadata": {
        "id": "S_G9S1Eqysc8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generateTrainSet(np_train_toxic, np_train_non_toxic, toxic_count):\n",
        "  x_train_toxic = np.asarray(np_train_toxic[:, 2])\n",
        "  y_train_toxic = np.asarray(np_train_toxic[:, 1])\n",
        "  \n",
        "  x_train_non_toxic = []\n",
        "  y_train_non_toxic = []\n",
        "  \n",
        "  for i in range(toxic_count):\n",
        "    choice = random.choice(np_train_non_toxic)\n",
        "    x_train_non_toxic.append(choice[2])\n",
        "    y_train_non_toxic.append(choice[1])\n",
        "  \n",
        "  x_train_non_toxic = np.asarray(x_train_non_toxic)\n",
        "  y_train_non_toxic = np.asarray(y_train_non_toxic)\n",
        "  \n",
        "  x_train = np.concatenate((x_train_toxic, x_train_non_toxic)).reshape((toxic_count * 2, 1))\n",
        "  del x_train_toxic, x_train_non_toxic\n",
        "  x_train = np.asarray(x_train)\n",
        "  \n",
        "  y_train = np.concatenate((y_train_toxic, y_train_non_toxic)).reshape((toxic_count * 2, 1))\n",
        "  del y_train_toxic, y_train_non_toxic\n",
        "  y_train = np.where(np.asarray(y_train) >= 0.5, 1, 0)\n",
        "  \n",
        "  data = np.concatenate((x_train, y_train), axis=1)\n",
        "  \n",
        "  np.random.shuffle(data)\n",
        "  return data[:, 0], data[:, 1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CagpUE9wT9lv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Extract x_train, y_train, x_valid, y_valid and x_test from the numpy arrays.\n",
        "The train dataset will be split into train and validation set to measure the performance of the model."
      ]
    },
    {
      "metadata": {
        "id": "eGrgR_4EUPgw",
        "colab_type": "code",
        "outputId": "1be7e393-576b-4d03-f38f-a848bba474cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "x_train = np_train[:, 2]\n",
        "y_train = np_train[:, 1]\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.40, shuffle=True)\n",
        "\n",
        "print(f'The len of the train set is: {x_train.shape[0]}')\n",
        "print(f'The len of the validation set is: {x_valid.shape[0]}')\n",
        "\n",
        "y_train = np.where(y_train >= 0.5, 1, 0)\n",
        "y_valid = np.where(y_valid >= 0.5, 1, 0)\n",
        "\n",
        "x_test = np_test[:, 1]\n",
        "x_test_ids = np_test[:, 0]"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The len of the train set is: 907876\n",
            "The len of the validation set is: 605251\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hxDC5oCalM6z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Now we have the train set, test set and validation set. We have also created word embeddings for the known words by GloVe embeddings and added random embeddings for unknown words including SOS, EOS, PAD."
      ]
    },
    {
      "metadata": {
        "id": "eTfyhEjPsuZ8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Creating sequences from words.\n",
        "Some functions to create sequences of indexes for a given sentence. The output will be a Pytorch tensor."
      ]
    },
    {
      "metadata": {
        "id": "QJm_Fmons4be",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def indexesFromSentence(sentence):\n",
        "  words = []\n",
        "  for word in sentence.split():\n",
        "    try:\n",
        "      words.append(word2idx[word])\n",
        "    except KeyError:\n",
        "      words.append(word2idx['UNK'])\n",
        "  return words  \n",
        "    \n",
        "#     return [word2idx[word] for word in sentence.split(' ')]\n",
        "    \n",
        "def tensorFromSentence(sentence):\n",
        "    indexes = indexesFromSentence(sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    if len(indexes) < MAX_LENGTH:\n",
        "      for i in range(len(indexes), MAX_LENGTH):\n",
        "        indexes.append(PAD_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0J615uhFm9lm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "14354ed7-ad2b-40b8-813c-1fd854d9d220"
      },
      "cell_type": "code",
      "source": [
        "print(f'Vocabulary size according to word list: {len(words)}')\n",
        "print(f'Vocabulary size of embeddings vector: {len(embeddings)}')\n",
        "print(f'Number of dimensions of embeddings: {len(embeddings[400003])}')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size according to word list: 400004\n",
            "Vocabulary size of embeddings vector: 400004\n",
            "Number of dimensions of embeddings: 50\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AWdAXBObBPf7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Define the GRU model.\n",
        "The model will take an input word's index, fetch up the corresponding embedding and then run through a Gated Recurrent Unit to return an Output and a Hidden State. The output will further be passed on to an ANN."
      ]
    },
    {
      "metadata": {
        "id": "xEAPp1j8BuHf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class GRU(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dims, hidden_size, num_layers):\n",
        "    super(GRU, self).__init__()\n",
        "    self.vocab_size = vocab_size\n",
        "    self.embedding_dims = embedding_dims\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_layers = num_layers\n",
        "#     embedding_weights = torch.tensor(embeddings, dtype=torch.float)\n",
        "#     self.embedding = nn.Embedding.from_pretrained(embedding_weights)\n",
        "#     self.embedding = nn.Embedding(self.vocab_size, self.embedding_dims)\n",
        "    self.gru = nn.GRU(self.embedding_dims, self.hidden_size, num_layers=self.num_layers)\n",
        "  \n",
        "  def forward(self, input, hidden_state):\n",
        "#     embedded = self.embedding(input).view(1, 1, -1) # Output of size -> 1, 1, embedding_dims\n",
        "#     output = embedded\n",
        "    input = input.view(1, 1, -1)\n",
        "    # dims of hidden_state = 2, 1, hidden_size (2 because 2 layers)\n",
        "    output, hidden_state = self.gru(input, hidden_state)\n",
        "    return output, hidden_state\n",
        "  \n",
        "  def initHidden(self):\n",
        "    return torch.randn(self.num_layers, 1, self.hidden_size, device=device)\n",
        "    \n",
        "    \n",
        "class ANN(nn.Module):\n",
        "  def __init__(self, input_size):\n",
        "    super(ANN, self).__init__()\n",
        "    self.fc1 = nn.Linear(input_size, 128)\n",
        "    self.fc2 = nn.Linear(128, 64)\n",
        "    self.fc3 = nn.Linear(64, 2)\n",
        "    self.softmax = nn.LogSoftmax(dim=1)\n",
        "    \n",
        "  def forward(self, input):\n",
        "    output = F.dropout(F.relu(self.fc1(input)), p=0.2)\n",
        "    output = F.dropout(F.relu(self.fc2(output)), p=0.2)\n",
        "    output = self.fc3(output)\n",
        "#     print(f'Before softmax: {output}')\n",
        "    output = self.softmax(output)\n",
        "    return output\n",
        "\n",
        "# class ANN(nn.Module):\n",
        "#   def __init__(self, input_size):\n",
        "#     super(ANN, self).__init__()\n",
        "#     self.fc1 = nn.Linear(input_size, 128)\n",
        "#     self.fc2 = nn.Linear(128, 2)\n",
        "#     self.softmax = nn.LogSoftmax(dim=1)\n",
        "    \n",
        "#   def forward(self, input):\n",
        "#     output = F.dropout(F.relu(self.fc1(input)), p=0.2)\n",
        "#     output = self.fc2(output)\n",
        "# #     print(f'Before softmax: {output}')\n",
        "# #     output = self.softmax(output)\n",
        "#     return output\n",
        "  \n",
        "# for i in range(10):\n",
        "#   input_tensor = tensorFromSentence(x_train[i])\n",
        "#   hidden = gru.initHidden()\n",
        "#   loss = 0\n",
        "#   for index in range(input_tensor.shape[0]):\n",
        "#     output, hidden = gru(input_tensor[index], hidden)\n",
        "    \n",
        "#   y_pred = ann(output)\n",
        "# #     y_pred = nn.LogSoftmax(y_pred)\n",
        "# #   print(f'after softmax: {y_pred}')\n",
        "# #     topv, topi = y_pred.topk(1)\n",
        "# #     print(f'topv: {topv[0][0][0]}, topi: {topi}')\n",
        "# #     print(y_pred, torch.tensor(y_train[2], device=device))\n",
        "# #     loss += criterion(topv.view(1, -1), torch.tensor(y_train[index], device=device).view(1, ))\n",
        "#   loss += criterion(y_pred.view(1, -1), torch.tensor(y_train[index], device=device).view(1, ))\n",
        "#   loss.backward()\n",
        "#   gru_optimizer.step()\n",
        "#   ann_optimizer.step()\n",
        "#   print(f'loss: {loss}')\n",
        "#   loss = 0\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KAEtftmWFxdQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Some functions to find time elapsed\n",
        "These functions help to calculate the elapsed time and the remaining time."
      ]
    },
    {
      "metadata": {
        "id": "A205i9dvFvHF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0H0pRLRPHuNH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training a single sentence."
      ]
    },
    {
      "metadata": {
        "id": "MhMAQJ6R5DpA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def trainSentence(input_tensor, target_label, gru, ann, gru_optimizer, ann_optimizer, criterion):\n",
        "  \n",
        "  \n",
        "  hidden_state = gru.initHidden()\n",
        "  \n",
        "  gru_optimizer.zero_grad()\n",
        "  ann_optimizer.zero_grad()\n",
        "  \n",
        "  loss = 0\n",
        "  # pass the whole sentence through the rnn. \n",
        "  for index in range(input_tensor.shape[0]):\n",
        "    word_embedding = []\n",
        "    try:\n",
        "#       print(f'here: {embeddings[input_tensor[index]]}')\n",
        "      word_embedding = torch.FloatTensor(embeddings[index]).to(device)\n",
        "#       print('here too')\n",
        "    except KeyError:\n",
        "      word_embedding = torch.FloatTensor(embeddings[word2idx['UNK']]).to(device)\n",
        "#     output, hidden_state = gru(input_tensor[index], hidden_state)\n",
        "    output, hidden_state = gru(word_embedding, hidden_state)\n",
        "#     print(output[0])\n",
        "  \n",
        "  # feed the output tensor to the ann.\n",
        "  y_pred = ann(output)\n",
        "  print(f'y_pred: {y_pred}')\n",
        "  loss += criterion(y_pred.view(1, -1), torch.tensor(target_label, device=device).view(1, ))\n",
        "  \n",
        "  loss.backward()\n",
        "  gru_optimizer.step()\n",
        "  ann_optimizer.step()\n",
        "  \n",
        "  return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "F6xJ5rjpUdJG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Save model checkpoints after an interval.\n",
        "Save the model checkpoints to the drive and load from there whenever resuming training."
      ]
    },
    {
      "metadata": {
        "id": "p1LiB6aJUzfl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def save_model(model, optimizer, model_name, iter, print_loss_total, plot_loss_total, plot_losses):\n",
        "  path = f'/content/gdrive/My Drive/Machine Learning Models/new_kaggle_jigsaw_{model_name}_max_len_220.pth'\n",
        "  \n",
        "  # Check if a previous model exists and then delete it.\n",
        "  saved_models = os.listdir('gdrive/My Drive/Machine Learning Models/')\n",
        "  if os.path.exists(path):\n",
        "    print('Deleting existing model')\n",
        "    os.remove(path)\n",
        "  \n",
        "  print(f'Saving {model_name} model...')\n",
        "  torch.save({'iteration': iter,\n",
        "             'model_state_dictionary': model.state_dict(),\n",
        "             'optimizer_state_dictionary': optimizer.state_dict(),\n",
        "             'print_loss_total': print_loss_total,\n",
        "             'plot_loss_total': plot_loss_total,\n",
        "             'plot_losses': plot_losses}, path)\n",
        "  print(f'{model_name} saved successfully.')\n",
        "  \n",
        "def load_model(model_name):\n",
        "  path = f'/content/gdrive/My Drive/Machine Learning Models/new_kaggle_jigsaw_{model_name}_max_len_220.pth'\n",
        "  if os.path.exists(path):\n",
        "    checkpoint = torch.load(path)\n",
        "    return checkpoint\n",
        "  else:\n",
        "    return None\n",
        " \n",
        "# save_model(gru, gru_optimizer, 'gru', 1000, 100, 1000, 100)\n",
        "\n",
        "# checkpoint = load_model('gru')\n",
        "# print(type(checkpoint['print_loss_total']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ctFISipAE9lK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def trainIters(gru, ann, epochs = 3, print_every=5000, save_every = 6000, plot_every=5000, learning_rate=0.01, resume=False):\n",
        "  print('Training model...')\n",
        "  \n",
        "  for j in range(epochs):\n",
        "    print(f'EPOCH {j+1}')\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0\n",
        "    plot_loss_total = 0\n",
        "    \n",
        "    x_train, y_train = generateTrainSet(np_train_toxic, np_train_non_toxic, toxic_count)\n",
        "\n",
        "#     n_iters = x_train.shape[0]\n",
        "    n_iters = 1\n",
        "    print('Total number of iteration: ' + str(n_iters))\n",
        "    current_iteration = 1\n",
        "\n",
        "    if resume == True:\n",
        "      gru_checkpoint = load_model('gru')\n",
        "      ann_checkpoint = load_model('ann')\n",
        "\n",
        "      current_iteration = gru_checkpoint['iteration'] if gru_checkpoint != None else 1\n",
        "      gru.load_state_dictionary = gru_checkpoint['model_state_dictionary']\n",
        "      gru.train()\n",
        "      gru.to(device)\n",
        "      ann.load_state_dictionary = ann_checkpoint['model_state_dictionary']\n",
        "      ann.train()\n",
        "      ann.to(device)\n",
        "      gru_optimizer.load_state_dictionary = gru_checkpoint['optimizer_state_dictionary']\n",
        "      ann_optimizer.load_state_dictionary = ann_checkpoint['optimizer_state_dictionary']\n",
        "\n",
        "      plot_losses = gru_checkpoint['plot_losses']\n",
        "      print_loss_total = gru_checkpoint['print_loss_total']\n",
        "      plot_loss_total = gru_checkpoint['plot_loss_total']\n",
        "  \n",
        "    for iter in range(current_iteration, n_iters+1):\n",
        "      input_tensor = tensorFromSentence(x_train[iter-1])\n",
        "\n",
        "      loss = trainSentence(input_tensor, y_train[iter-1], gru, ann, gru_optimizer, ann_optimizer, criterion)\n",
        "\n",
        "      print_loss_total = print_loss_total + loss\n",
        "      plot_loss_total = plot_loss_total + loss\n",
        "\n",
        "      if iter % save_every == 0:\n",
        "        save_model(gru, gru_optimizer, 'gru', iter, print_loss_total, plot_loss_total, plot_losses)\n",
        "        save_model(ann, ann_optimizer, 'ann', iter, print_loss_total, plot_loss_total, plot_losses)\n",
        "        print(f'models saved after iteration: {iter}')\n",
        "\n",
        "      if iter % print_every == 0:\n",
        "        print_loss_avg = print_loss_total / print_every\n",
        "        print_loss_total = 0\n",
        "        print(f'Epoch: {j+1} Time since: {timeSince(start, iter/n_iters)}; Iteration: {iter}; Percentage elapsed: {(iter/n_iters)*100}; Loss: {print_loss_avg}')\n",
        "\n",
        "      if iter % plot_every == 0:\n",
        "        plot_loss_avg = plot_loss_total / plot_every\n",
        "        plot_losses.append(plot_loss_avg)\n",
        "        plot_loss_total = 0\n",
        "      \n",
        "  showPlot()\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o5XNnIIoGZcA",
        "colab_type": "code",
        "outputId": "bda9b743-171c-44b6-f0b3-3681cd1721e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        }
      },
      "cell_type": "code",
      "source": [
        "EMBEDDING_DIMS = 50\n",
        "HIDDEN_SIZE = 64\n",
        "    \n",
        "gru = GRU(len(words), EMBEDDING_DIMS, HIDDEN_SIZE, num_layers=1).to(device)\n",
        "ann = ANN(HIDDEN_SIZE).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "gru_optimizer = optim.SGD(gru.parameters(), lr=0.01)\n",
        "ann_optimizer = optim.SGD(ann.parameters(), lr=0.01)\n",
        "\n",
        "trainIters(gru, ann, print_every=500, resume=False)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training model...\n",
            "EPOCH 1\n",
            "Total number of iteration: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-216acc6b130a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mann_optimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mann\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtrainIters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgru\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mann\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresume\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-19-7e9770c3846a>\u001b[0m in \u001b[0;36mtrainIters\u001b[0;34m(gru, ann, epochs, print_every, save_every, plot_every, learning_rate, resume)\u001b[0m\n\u001b[1;32m     37\u001b[0m       \u001b[0minput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensorFromSentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainSentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgru\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mann\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgru_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mann_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m       \u001b[0mprint_loss_total\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprint_loss_total\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-51-9c4c4bb3c6ac>\u001b[0m in \u001b[0;36mtrainSentence\u001b[0;34m(input_tensor, target_label, gru, ann, gru_optimizer, ann_optimizer, criterion)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m#       print(f'here: {embeddings[input_tensor[index]]}')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m       \u001b[0mword_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;31m#       print('here too')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: too many dimensions 'str'"
          ]
        }
      ]
    }
  ]
}